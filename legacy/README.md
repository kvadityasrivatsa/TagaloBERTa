# Legacy Methods

This README documents the different methods for labelled-data-augmentation and hate-speech tagging approaches used in the current as well as earlier versions of the project.

### Contents

1. [Data](#Data)
   1. [Cleaning](#Cleaning)
   2. [Label Discrepancy](#Label-Discrepancy)
   3. [Iterative Label Extrapolation](#Iterative-Label-Extrapolation)
2. [Hate Speech Detection](#Hate-Speech-Detection)
   1. [Traditional Methods](#Traditional-Methods)
      1. [TF-IDF](#TF-IDF)
      2. [Ensemble Tree Classifiers](#Ensemble-Tree-Classifiers)
   2. [DL Methods](#DL-Methods)
      1. [Finetuning Transformer models](#Finetuning-Transformer-models)
      2. [Re-Training Transformer models](#Re-Training-Transformer-models)
      3. [Training Static-Embeddings (GloVe) from scratch](#Training-Static-Embeddings-(GloVe)-from-scratch)
      4. [Training RoBERTa from scratch (TagaloBERTa)](#Training-RoBERTa-from-scratch-(TagaloBERTa))
   3. [Prediction Ratio Amendments](#Prediction-Ratio-Amendments)
3. [Conclusion](#Conclusion)



## Data

The data consists of code-mixed Tagalog-English comments. The tagged data contains over 110,000 annotations (individual 0/1 tags) with about 11k unique sentences. The raw data contains around 400 Million samples, of which 30M comments were sampled for various augmentation and training subtasks in this project.

### Cleaning

The [cleaning script](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/c72540858458b5d70c8ef98250238926e115844b/preprocessing.py#L10) removes the following elements in the comment text (in the given order):

1. URLs: `(https?:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)`
2. Non-Alphanumerics: `[^a-zA-Z0-9']` (NOTE: If extending the code to a different data, a pattern with more coverage is required).
3. Multiple spaces
4. Bounding characters (at either ends of the text): `[' ','\t','\n']`
5. Lengthy sequences: First 512 characters selected from each comment text.

[Return to top](#Contents)

### Label Discrepancy

The above mentioned 110,000 annotations range over 11k unique comment texts, but the the number of annotations per comment vary widely  (as shown in the figure below)

<p align="center">
<img src="https://i.imgur.com/VQUUnYP.png" width="700" />
</p>

This creates the need for a policy for regarding sequences as (in)valid based on the number of annotations they have as well as which kind. For the data files used in this project, the following policy was employed:

```python
valid if annotation.length > 2:
{
	True if annotations.positive / annotations.length > k;
	else False
}
```

Sampling was done with k set as `k = {0.5,0.4,0.3,0.2}`

Additionally, for each data subset generated by the policy above, two kinds of files were generated:

- `greater`: In which all samples simply comply with the >k condition.
- `balanced`: In which, the number of instances of the majority class (in this case negative (0)) was limited by the number of instances of the minority class, thereby having equal instances of each class.

The above data files have been generated and stored with the following nomenclature:

`(greater/balanced)_<k>.tsv`

`greater.tsv` and `balanced.tsv` assume k as 0.5.

**Label Discrepancy Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1caHPZCSxCHdtSxV26wkg71dGB1wDtAfE?usp=sharing)

[Return to top](#Contents)

### Iterative Label Extrapolation

The above filtering generates data files with far fewer samples (2k-4k) than the original labeled dataset (11k). This can lead to a quick over-fit over the small data while using present day DL models. To curb this, an iterative approach was employed to generate pseudo-labels for untagged samples from the raw data (30M set). 

The aim was to identify those samples which qualify as positive(0) or negative(1) with a substantial confidence based on the current set of labeled samples using a rule based predictor, and thus can be assumed to have the predicted label. The set of samples classified in one iteration can be used as the labeled set for the next iteration, thereby generating labels for newer samples from the raw dataset.

```python
# Iterative Label Extrapolation
Iteration:
{
	# 1. generate class-sensitive vocabularies
	pos_voc = {'w': no. of times 'w' occurs in positively labeled samples / length of pos_voc}
	neg_voc = {'w': no. of times 'w' occurs in negatively labeled samples / length of neg_voc}
	
	# 2. 
}
```

**Iterative Label Extrapolation Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11285V_ajtLbgKqULFRvejdrFXbIdeTR6?usp=sharing)

[Return to top](#Contents)

## Hate Speech Detection

### Traditional Methods

#### TF-IDF

[TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) was used as a preliminary method to generate Bag-of-Words features for classification. 

The default vectorizer setting assumes unigram level tokens. However, as the data consists of both English and Tagalog, a parallel setting with a [SentencePiece Tokenizer](https://github.com/google/sentencepiece) (vocab size=5000) was applied to the text before running the vectorizer. The version of data used for this phase was preprocessed using the aforementioned [cleaning](#Cleaning) and a simple label policy (`True if annotations.positive.count >= annotations.negative.count else False`) only.

#### Ensemble Tree Classifiers

The features generated by TF-IDF were passed to tree based sparse classifiers, namely: 

1. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
2. [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)

Both classifiers were trained on their default hyperparameter settings.

The overall accuracy of TF-IDF + (Random Forest / XGBoost) ranged between 64% to 71%:

**Random Forest**

```
# Without class balancing
                precision    recall  f1-score   support

             0       0.71      0.97      0.82      2121
             1       0.71      0.16      0.26      1016

      accuracy                           0.71      3137
     macro avg       0.71      0.56      0.54      3137
  weighted avg       0.71      0.71      0.64      3137

# With class balancing
                precision    recall  f1-score   support

             0       0.72      0.89      0.80      2121
             1       0.55      0.28      0.37      1016

      accuracy                           0.69      3137
     macro avg       0.63      0.58      0.58      3137
  weighted avg       0.66      0.69      0.66      3137
```
**XGBoost**

```
# Without class balancing
                precision    recall  f1-score   support

             0       0.71      0.98      0.82      2121
             1       0.80      0.14      0.24      1016

      accuracy                           0.71      3137
     macro avg       0.75      0.56      0.53      3137
  weighted avg       0.73      0.71      0.63      3137

# With class balancing
                precision    recall  f1-score   support

             0       0.75      0.70      0.72      2121
             1       0.45      0.50      0.47      1016

      accuracy                           0.64      3137
     macro avg       0.60      0.60      0.60      3137
  weighted avg       0.65      0.64      0.64      3137
```

**Traditional Methods Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SlYKqTedG0R4XM1VwEmIxcmQVsrR53pv?usp=sharing)

[Return to top](#Contents)

### DL Methods

#### Finetuning Transformer models

This is normally the go-to method (and approached second after traditional methods in this project) for sequence-tagging. It involves training a transformer encoder with a classification head on top (for this project: a linear layer with dropout).

1. **XLMRoBERTa** 
   - Several papers mentioned the use of XLMRoBERTa for multilingual (and possibly code-mixed) data, however, it was quickly determined that the portion of data used for encoder training from the primary target langugage (Tagalog) was insufficient to produce reasonable embeddings for the pipeline and largely focussed on the English tokens of the hate speech-tagging data.
2. **RoBERTa** ([jcblaise/roberta-tagalog-base](https://huggingface.co/jcblaise/roberta-tagalog-base))
   - The model is pre-trained on TLUnified (large Filipino corpus) and released on HuggingFace.

**Finetuning Notebook (Original codebase):** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JEdzwgFiw9Vhr9-izSHGrh7eVoB4X3qv?usp=sharing) 

**NOTE:** The above script is an older version used primarily for fine-tuning the jcblaise RoBERTa. The finetuning script used in the latest version of the project is given below.

**Finetuning Notebook (Revised codebase, used alongside updated dataset):** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ufIDIYU4LDYytbwF2S2D05NXTe221W_Y?usp=sharing) 

[Return to top](#Contents)

#### Re-Training Transformer models

Considering the poor performance on purely finetuned models, the next step was to further train (or re-train) on raw text resembling the distribution of the annotated data. RoBERTa ([jcblaise/roberta-tagalog-base](https://huggingface.co/jcblaise/roberta-tagalog-base)) was re-trained on a 1M sample set from the raw corpus as well as the coment text column from the annotated data (NOTE: This does not lead to invalid testing later, text samples used from the annotated data are trained on the MLM task, and are not accompanied by the respective sample labels during this re-training).

One of the issues identified with the data was the difference in the subword token distribution between a largely Filipino corpus (TLUnified) and a code-mixed Filipino-English corpus. Thus, simply re-training would not affect the pre-trained tokenizer (i.e. subword token distribution) used to segment raw text for re-training. This suggested the need for <u>training an encoder from scratch</u>. 

**Re-Training RoBERTa Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1b7T580HZhAg50hvl0A2Sq7ATu50gCNeR?usp=sharing)

[Return to top](#Contents)

#### Training Static-Embeddings (GloVe) from scratch

As training a Transformer based model from scratch is data intensive, an intermediate apporach was taken up to train a static-embeddings language model. GloVe embeddings were used for this. 

**Results:** The embeddings from the GloVe model coupled with the linear layer finetuning as well as sparse-forest classification gave similar scores as the traditional approaches step. The poor performance can largely be attributed to the data being used at the time (same as traditional methods: highly unbalanced samples or too few balanced examples (~2k)).

**GloVe Training Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17gkNtYevalFuyvJL5zNKSvKUQFSqnEGv?usp=sharing)

[Return to top](#Contents)

#### Training RoBERTa from scratch (TagaloBERTa)

In the project timeline, parallel to the iterative label extrapolation, RoBERTa models were trained from scratch on a large raw code-mixed corpora. Three models were trained with the training corpus sizes: 1M, 10M, and 30M. These can be located at [TagaloBERTa/base_models](https://drive.google.com/drive/folders/1YHdJiwcWaKXFbPx3O7EG89CSfjBhRPeI?usp=sharing).

The final pipeline utilizes the RoBERTa model pre-trained on the 30M set for further finetuning for the classification task.

**Training LM code:** [TagaloBERTa/train_lm.py](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/main/train_lm.py)

[Return to top](#Contents)

### Prediction Ratio Amendments

The initial release of the pipeline consisted of a RoBERTa model with a linear tuning head, pre-trained on the 30M set, and finetuned on the combined set of `balanced_10.tsv` and the standalone [huggingface dataset](https://huggingface.co/datasets/hate_speech_filipino). The model returned the following scores on the heldout test set:

```
              precision    recall  f1-score   support

           0       0.92      0.93      0.93      1267
           1       0.93      0.92      0.92      1133

    accuracy                           0.93      2400
   macro avg       0.93      0.92      0.92      2400
weighted avg       0.93      0.93      0.92      2400
```

However, upon running inference on the 400M raw samples, it was found that approx. 56% of the samples were flagged as hateful (tag-1). This suggests too loose a bound on the class. Further analysis showed that on a random 1M sample set:

- Finetuning on `balanced_10.tsv`  resulted in ~27% samples being assigned class-1.
- On the standalone huggingface dataset: ~25%.
- The annotation-intersection of the above two datasets (`label = AND(balaced.label,hgfc.label)`): ~24%.

The above combinations still maintained a low pass for the positive class. To make the class boundary further stringent, the originally used TF-IDF driven Random Forest model ([code section](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/29d88d2ea9c304d617509445309d741dc376d8b0/generate_labels.py#L85)) was also applied in intersection. As the model is purely lexical, despite a slightly poorer classification performance, the statistical model is capable of identifying key hateful tokens. 

<u>NOTE:</u> The Random Forest model mentioned here is trained on the revised data, unlike the [original RF model](#Ensemble-Tree-Classifiers). Thus the revised scores on a heldout test set (for the RF model only) are given below:

```
              precision    recall  f1-score   support

           0       0.89      0.94      0.92      6729
           1       0.82      0.70      0.76      2492

    accuracy                           0.88      9221
   macro avg       0.86      0.82      0.84      9221
weighted avg       0.87      0.88      0.87      9221
```

The tags from this model and that from the TagaloBERTa model were combined using a logical-AND. This retrains most of the hateful comments while discarding most false positives. 

The final positve class proportion amounted to roughly **12%**.

**Intersection Random Forest Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](**GloVe Training Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17gkNtYevalFuyvJL5zNKSvKUQFSqnEGv?usp=sharing))

[Return to top](#Contents)

### Conclusion

The issues faced in this project can be linked to two major root causes:

1. Too few quality annotation labels:

   This was eventually addressed using [iterative label extrapolation](#Iterative-Label-Extrapolation) as well as adding [external annotated data](https://huggingface.co/datasets/hate_speech_filipino) to improve the volumne as well as quality of the training data.

2. Lack of pre-trained encoders specific to the code-mixed Tagalog-English data present in labelled exmaples:

   Following an incremental process, from finetuning and re-training, to pre-training a RoBERTa model from scratch, TagaloBERTa was trained on the comment texts from the labelled as well as the raw (~30M) samples mathcing the distribution of content as well as extent of code-mixing as in the original training data.

Other causes:

- Loose class bound:

  Individual models trained/finetuned on the annotated data ended up calssifying substantially more comments as positive (hateful) than necessary when applied to previously unseen data. This was curbed by making the class bound more stringent, by passing the label predictions of the individual models through a logical-AND pass.

The final pipeline utilizes the augmented labelled data to fine-tune a (linear) classification head on a pre-trained RoBERTa model as well as a Random Forest Classifier trained on TF-IDF features from the same data as used for the aforementioned funtuning.

The code and instructions for generating prediction labels on comment text can be found [here](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/main/generate_labels.py).

[Return to top](#Contents)

