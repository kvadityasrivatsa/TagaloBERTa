# Legacy Methods

This README documents the different methods for labelled-data-augmentation and hate-speech tagging approaches used in the current as well as earlier versions of the project.

### Contents

1. [Data](#Data)
   1. [Cleaning](#Cleaning)
   2. [Label Discrepancy](#Label-Discrepancy)
   3. [Iterative Label Extrapolation](#Iterative-Label-Extrapolation)
2. [Hate Speech Detection](#Hate-Speech-Detection)
   1. [Traditional Methods](#Traditional-Methods)
      1. [TF-IDF](#TF-IDF)
      2. [Ensemble Tree Classifiers](#Ensemble-Tree-Classifiers)
   2. [DL Methods](#DL-Methods)
      1. [Finetuning Transformer models](#Finetuning-Transformer-models)
      2. [Re-Training Transformer models](#Re-Training-Transformer-models)
      3. [Training Static-Embeddings (GloVe) from scratch](#Training-Static-Embeddings-(GloVe)-from-scratch)
      4. [TagaloBERTa: Training RoBERTa from scratch](#TagaloBERTa:-Training-RoBERTa-from-scratch)



## Data

The data consists of code-mixed Tagalog-English comments. The tagged data contains over 110,000 annotations (individual 0/1 tags) with about 11k unique sentences. The raw data contains around 400 Million samples, of which 30M comments were sampled for various augmentation and training subtasks in this project.

### Cleaning

The [cleaning script](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/c72540858458b5d70c8ef98250238926e115844b/preprocessing.py#L10) removes the following elements in the comment text (in the given order):

1. URLs: `(https?:\/\/)?([\da-z\.-]+)\.([a-z\.]{2,6})([\/\w \.-]*)`
2. Non-Alphanumerics: `[^a-zA-Z0-9']` (NOTE: If extending the code to a different data, a pattern with more coverage is required).
3. Multiple spaces
4. Bounding characters (at either ends of the text): `[' ','\t','\n']`
5. Lengthy sequences: First 512 characters selected from each comment text.

[Return to top](#Contents)

### Label Discrepancy

The above mentioned 110,000 annotations range over 11k unique comment texts, but the the number of annotations per comment vary widely  (as shown in the figure below)

<p align="center">
<img src="https://i.imgur.com/VQUUnYP.png" width="700" />
</p>
	
This creates the need for a policy for regarding sequences as (in)valid based on the number of annotations they have as well as which kind. For the data files used in this project, the following policy was employed:

```python
valid if annotation.length > 2:
{
	True if annotations.positive / annotations.length > k;
	else False
}
```

Sampling was done with k set as `k = {0.5,0.4,0.3,0.2}`

Additionally, for each data subset generated by the policy above, two kinds of files were generated:

- `greater`: In which all samples simply comply with the >k condition.
- `balanced`: In which, the number of instances of the majority class (in this case negative (0)) was limited by the number of instances of the minority class, thereby having equal instances of each class.

The above data files have been generated and stored with the following nomenclature:

`(greater/balanced)_<k>.tsv`

`greater.tsv` and `balanced.tsv` assume k as 0.5.

**Label Discrepancy Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1caHPZCSxCHdtSxV26wkg71dGB1wDtAfE?usp=sharing)

[Return to top](#Contents)

### Iterative Label Extrapolation

The above filtering generates data files with far fewer samples (2k-4k) than the original labeled dataset (11k). This can lead to a quick over-fit over the small data while using present day DL models. To curb this, an iterative approach was employed to generate pseudo-labels for untagged samples from the raw data (30M set). 

The aim was to identify those samples which qualify as positive(0) or negative(1) with a substantial confidence based on the current set of labeled samples using a rule based predictor, and thus can be assumed to have the predicted label. The set of samples classified in one iteration can be used as the labeled set for the next iteration, thereby generating labels for newer samples from the raw dataset.

```python
# Iterative Label Extrapolation
Iteration:
{
	# 1. generate class-sensitive vocabularies
	pos_voc = {'w': no. of times 'w' occurs in positively labeled samples / length of pos_voc}
	neg_voc = {'w': no. of times 'w' occurs in negatively labeled samples / length of neg_voc}
	
	# 2. 
}
```

**Iterative Label Extrapolation Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/11285V_ajtLbgKqULFRvejdrFXbIdeTR6?usp=sharing)

[Return to top](#Contents)

## Hate Speech Detection

### Traditional Methods

#### TF-IDF

[TF-IDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) was used as a preliminary method to generate Bag-of-Words features for classification. 

The default vectorizer setting assumes unigram level tokens. However, as the data consists of both English and Tagalog, a parallel setting with a [SentencePiece Tokenizer](https://github.com/google/sentencepiece) (vocab size=5000) was applied to the text before running the vectorizer. The version of data used for this phase was preprocessed using the aforementioned [cleaning](#Cleaning) and a simple label policy (`True if annotations.positive.count >= annotations.negative.count else False`) only.

#### Ensemble Tree Classifiers

The features generated by TF-IDF were passed to tree based sparse classifiers, namely: 

1. [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
2. [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)

Both classifiers were trained on their default hyperparameter settings.

The overall accuracy of TF-IDF + (Random Forest / XGBoost) ranged between 64% to 71%:

**Random Forest**

```
	# Without class balancing
                    precision    recall  f1-score   support

                 0       0.71      0.97      0.82      2121
                 1       0.71      0.16      0.26      1016

          accuracy                           0.71      3137
         macro avg       0.71      0.56      0.54      3137
      weighted avg       0.71      0.71      0.64      3137
      
	# With class balancing
                    precision    recall  f1-score   support

                 0       0.72      0.89      0.80      2121
                 1       0.55      0.28      0.37      1016

          accuracy                           0.69      3137
         macro avg       0.63      0.58      0.58      3137
      weighted avg       0.66      0.69      0.66      3137
```
**XGBoost**

```
	# Without class balancing
                    precision    recall  f1-score   support

                 0       0.71      0.98      0.82      2121
                 1       0.80      0.14      0.24      1016

          accuracy                           0.71      3137
         macro avg       0.75      0.56      0.53      3137
      weighted avg       0.73      0.71      0.63      3137

	# With class balancing
                    precision    recall  f1-score   support

                 0       0.75      0.70      0.72      2121
                 1       0.45      0.50      0.47      1016

          accuracy                           0.64      3137
         macro avg       0.60      0.60      0.60      3137
      weighted avg       0.65      0.64      0.64      3137
```

**Traditional Methods Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SlYKqTedG0R4XM1VwEmIxcmQVsrR53pv?usp=sharing)

[Return to top](#Contents)

### DL Methods

#### Finetuning Transformer models

This is normally the go-to method (and approached second after traditional methods in this project) for sequence-tagging. It involves training a transformer encoder with a classification head on top (for this project: a linear layer with dropout).

1. **XLMRoBERTa** 
   - Several papers mentioned the use of XLMRoBERTa for multilingual (and possibly code-mixed) data, however, it was quickly determined that the portion of data used for encoder training from the primary target langugage (Tagalog) was insufficient to produce reasonable embeddings for the pipeline and largely focussed on the English tokens of the hate speech-tagging data.
2. **RoBERTa** ([jcblaise/roberta-tagalog-base](https://huggingface.co/jcblaise/roberta-tagalog-base))
   - The model is pre-trained on TLUnified (large Filipino corpus) and released on HuggingFace.

**Finetuning Notebook:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JEdzwgFiw9Vhr9-izSHGrh7eVoB4X3qv?usp=sharing) 

**NOTE:** The above script is an older version used primarily for fine-tuning the jcblaise RoBERTa. The finetuning script used in the latest version of the project is located [here](), in the main repository.

[Return to top](#Contents)

#### Re-Training Transformer models

Considering the poor performance on purely finetuned models, the next step was to further train (or re-train) an existing encoder on the raw text from the tagged as well as untagged Tagalog-English comments corpus. RoBERTa ([jcblaise/roberta-tagalog-base](https://huggingface.co/jcblaise/roberta-tagalog-base)) was re-trained on <>.

[Return to top](#Contents)

#### Training Static-Embeddings (GloVe) from scratch

One of the issues identified with the data was the difference in the subword token distribution between a largely Filipino corpus (TLUnified) and a code-mixed Filipino-English corpus. Thus, simply re-training would not affect the pre-trained tokenizer (i.e. subword token distribution) used to segment raw text for re-training. This suggested the need for training an encoder from sratch. 

As training a Transformer based model from scratch is data intensive, an intermediate apporach was taken up to train a static-embeddings language model. GloVe embeddings were used for this. 

[Return to top](#Contents)

#### TagaloBERTa: Training RoBERTa from scratch

In the project timeline, parallel to the iterative label extrapolation, RoBERTa models were trained from scratch on a large raw code-mixed corpora. Three models were trained with the training corpus sizes: 1M, 10M, and 30M. These can be located at [TagaloBERTa/base_models](https://drive.google.com/drive/folders/1YHdJiwcWaKXFbPx3O7EG89CSfjBhRPeI?usp=sharing).

The training config used was the following:

```

```

**Training LM code:** [TagaloBERTa/train_lm.py](https://github.com/kvadityasrivatsa/TagaloBERTa/blob/main/train_lm.py)

[Return to top](#Contents)
